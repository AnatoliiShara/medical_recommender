# -*- coding: utf-8 -*-
import argparse
import sys
import re
import pandas as pd
from typing import List, Dict
sys.path.append('src')

from search.enhanced_medical_assistant import EnhancedMedicalAssistant, SearchConfig

# –ü—Ä–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç–∏ —Ç–∞ —Å–ø–∏—Å–∫–∏ –ø—ñ–¥—Ä—è–¥–∫—ñ–≤, —è–∫—ñ –≤–≤–∞–∂–∞—î–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–º–∏ –≤ "–ü–æ–∫–∞–∑–∞–Ω–Ω—è"
DEFAULT_QUERIES = {
    "–∞—Ä—Ç–µ—Ä—ñ–∞–ª—å–Ω–∞ –≥—ñ–ø–µ—Ä—Ç–µ–Ω–∑—ñ—è": ["–≥—ñ–ø–µ—Ä—Ç–µ–Ω–∑—ñ", "–∞—Ä—Ç–µ—Ä—ñ–∞–ª—å–Ω", "–≤–∏—Å–æ–∫–∏–π —Ç–∏—Å–∫", "–ø—ñ–¥–≤–∏—â–µ–Ω–∏–π —Ç–∏—Å–∫"],
    "—Å–µ—Ä—Ü–µ–≤–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—ñ—Å—Ç—å": ["—Å–µ—Ä—Ü–µ–≤ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω"],
    "—Ü—É–∫—Ä–æ–≤–∏–π –¥—ñ–∞–±–µ—Ç": ["—Ü—É–∫—Ä–æ–≤–∏–π –¥—ñ–∞–±–µ—Ç", "–¥—ñ–∞–±–µ—Ç"],
    "–≥–æ–ª–æ–≤–Ω–∏–π –±—ñ–ª—å": ["–≥–æ–ª–æ–≤–Ω –±—ñ–ª", "–º—ñ–≥—Ä–µ–Ω", "—Ü–µ—Ñ–∞–ª–≥—ñ"],
    "–∞—Å—Ç–º–∞": ["–∞—Å—Ç–º", "–±—Ä–æ–Ω—Ö—ñ–∞–ª—å–Ω–∞ –∞—Å—Ç–º–∞"],
}

def build_engine(df: pd.DataFrame, max_chunk_tokens: int) -> EnhancedMedicalAssistant:
    eng = EnhancedMedicalAssistant()
    eng.build_from_dataframe(
        df,
        encoder_model=None,
        medical_chunking=True,
        max_chunk_tokens=max_chunk_tokens
    )
    return eng

def is_relevant(row: pd.Series, substrings: List[str]) -> bool:
    txt = str(row.get("–ü–æ–∫–∞–∑–∞–Ω–Ω—è", "")).lower()
    return any(ss in txt for ss in substrings)

def ndcg_at_k(gains: List[int], k: int = 10) -> float:
    import math
    gains = gains[:k]
    if not gains:
        return 0.0
    dcg = sum((g / math.log2(i+2)) for i, g in enumerate(gains))
    ideal = sorted(gains, reverse=True)
    idcg = sum((g / math.log2(i+2)) for i, g in enumerate(ideal))
    return float(dcg / (idcg + 1e-9))

def recall_at_k(gains: List[int], total_relevant: int, k: int = 10) -> float:
    return float(min(sum(gains[:k]), total_relevant) / (total_relevant + 1e-9))

def evaluate(df: pd.DataFrame, queries: Dict[str, List[str]], rrf_values: List[int], k: int = 10, max_chunk_tokens: int = 128):
    # –ü–æ–ø–µ—Ä–µ–¥–Ω—å–æ –≤–∏–∑–Ω–∞—á–∏–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ doc_ids –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ–∫–∞–∑–∞–Ω—å
    gt: Dict[str, set] = {}
    for q, subs in queries.items():
        rel_ids = set()
        for idx, row in df.iterrows():
            if is_relevant(row, subs):
                rel_ids.add(idx)
        gt[q] = rel_ids

    print(f"\nEvaluating RRF alpha values: {rrf_values}  (k={k}, max_chunk_tokens={max_chunk_tokens})")
    results = []
    for alpha in rrf_values:
        eng = build_engine(df, max_chunk_tokens=max_chunk_tokens)
        cfg = SearchConfig(rrf_alpha=float(alpha), top_k=100, show=100, max_chunk_tokens=max_chunk_tokens)

        agg_ndcg, agg_recall, count = 0.0, 0.0, 0
        for q, subs in queries.items():
            res = eng.search(q, cfg)
            ranked_doc_ids = [r["doc_id"] for r in res]
            # –ú–∞—Ä–∫—É—î–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å
            rel_set = gt[q]
            gains = [1 if did in rel_set else 0 for did in ranked_doc_ids]
            ndcg = ndcg_at_k(gains, k=k)
            recall = recall_at_k(gains, total_relevant=len(rel_set), k=k)
            agg_ndcg += ndcg
            agg_recall += recall
            count += 1
        results.append((alpha, agg_ndcg / count, agg_recall / count))

    print("\nalpha\t nDCG@10\t Recall@10")
    for alpha, ndcg, recall in results:
        print(f"{alpha}\t {ndcg:.4f}\t\t {recall:.4f}")

    best = max(results, key=lambda x: (x[1], x[2]))
    print(f"\nüèÜ Best alpha by nDCG@10 (tie-break Recall@10): {best[0]}  nDCG@10={best[1]:.4f}  Recall@10={best[2]:.4f}")

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True, help="Path to compendium_all.parquet (–∞–±–æ –ø—ñ–¥–≤–∏–±—ñ—Ä–∫—É)")
    ap.add_argument("--rows", type=int, default=1000, help="–°–∫—ñ–ª—å–∫–∏ —Ä—è–¥–∫—ñ–≤ –≤–∑—è—Ç–∏ –¥–ª—è —à–≤–∏–¥–∫–æ—ó –æ—Ü—ñ–Ω–∫–∏")
    ap.add_argument("--k", type=int, default=10, help="k –¥–ª—è nDCG/Recall")
    ap.add_argument("--alpha", type=str, default="60,90,120", help="–°–ø–∏—Å–æ–∫ RRF alpha, –∫–æ–º–æ—é")
    ap.add_argument("--max_chunk_tokens", type=int, default=128, help="–õ—ñ–º—ñ—Ç —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–∞ —á–∞–Ω–∫")
    args = ap.parse_args()

    df = pd.read_parquet(args.parquet).head(args.rows).copy()
    rrf_values = [int(x) for x in args.alpha.split(",") if x.strip()]
    evaluate(df, DEFAULT_QUERIES, rrf_values, k=args.k, max_chunk_tokens=args.max_chunk_tokens)
