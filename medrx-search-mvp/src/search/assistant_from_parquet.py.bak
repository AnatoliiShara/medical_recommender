# src/search/assistant_from_parquet.py
from __future__ import annotations

import argparse
from search.fusion_runtime import fuse_candidates
import dataclasses
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple
import os
import sys
import json
import time
import math
import re
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import pyarrow.parquet as pq

# --- optional / external deps (fail-soft) ---
try:
    import faiss  # type: ignore
except Exception:
    faiss = None

try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None  # type: ignore

try:
    from FlagEmbedding import FlagReranker  # BAAI/bge-reranker-v2-m3
except Exception:
    FlagReranker = None  # type: ignore

try:
    import yaml
except Exception:
    yaml = None

# --- our modules (with safe fallbacks) ---
try:
    from search.query_reform.normalizer import normalize_query  # preferred
except Exception:
    # safe fallback
    def normalize_query(q: str) -> str:
        q = (q or "").strip()
        # простенька нормалізація
        q = q.replace("’", "'").replace("`", "'").replace("“", '"').replace("”", '"')
        q = re.sub(r"\s+", " ", q)
        return q.lower()

try:
    from search.query_reform.alias_expander import AliasExpander
except Exception:
    class AliasExpander:  # safe fallback
        def __init__(self, csv_path: Optional[str] = None):
            self.ok = bool(csv_path and os.path.exists(csv_path))
            self._map = {}  # brand->inn, inn->brand
            if self.ok:
                try:
                    # очікуємо 2 колонки: brand,inn (у твоєму файлі — саме так)
                    df = pd.read_csv(csv_path)
                    for _, r in df.iterrows():
                        b = str(r.get("brand", "")).strip()
                        i = str(r.get("inn", "")).strip()
                        if b and i:
                            self._map.setdefault(b.lower(), set()).add(i.lower())
                            self._map.setdefault(i.lower(), set()).add(b.lower())
                except Exception:
                    self.ok = False

        def expand(self, q: str, max_terms: int = 5) -> List[str]:
            if not self.ok:
                return []
            toks = list(dict.fromkeys(re.findall(r"[^\W_]+", q.lower(), flags=re.UNICODE)))
            alts: List[str] = []
            for t in toks:
                if t in self._map:
                    alts.extend(list(self._map[t]))
            alts = list(dict.fromkeys([a for a in alts if a not in toks]))
            if not alts:
                return []
            # згенеруємо один-два варіанти (щоб не роздувати CE)
            variant = q + " " + " ".join(alts[:max_terms])
            return [variant]

try:
    from search.priors.clinical_priors import ClinicalPriors
except Exception:
    ClinicalPriors = None  # type: ignore


# ------------------- helpers -------------------
def _now_ts() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def _json_default(o):
    # безпечна серіалізація numpy / pathlib / ін.
    if isinstance(o, (np.integer,)):
        return int(o)
    if isinstance(o, (np.floating,)):
        return float(o)
    if isinstance(o, (np.ndarray,)):
        return o.tolist()
    if isinstance(o, (Path,)):
        return str(o)
    return str(o)


def _ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def _first_existing_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    for c in candidates:
        if c in df.columns:
            return c
    return None


def _safe_to_list(x) -> List[int]:
    out: List[int] = []
    if isinstance(x, list):
        for v in x:
            if isinstance(v, (int, np.integer)):
                out.append(int(v))
            elif isinstance(v, str) and v.isdigit():
                out.append(int(v))
    return out


# ------------------- configs -------------------
@dataclass
class GateConfig:
    mode: str = "prefer"
    sections: List[str] = field(default_factory=lambda: ["Показання", "Спосіб застосування та дози"])
    weight: float = 0.1  # якщо десь використовується для м'якого бусту


@dataclass
class IntentPolicy:
    doc_topN: int = 400
    k_final: int = 300
    gate: GateConfig = field(default_factory=GateConfig)

    @staticmethod
    def from_yaml(path: Optional[str]) -> Dict[str, "IntentPolicy"]:
        # дефолтна політика
        default = {
            "default": IntentPolicy(),
            "indication": IntentPolicy(doc_topN=400, k_final=300, gate=GateConfig(mode="prefer")),
            "dosage": IntentPolicy(doc_topN=300, k_final=200, gate=GateConfig(mode="prefer")),
            "contra": IntentPolicy(doc_topN=400, k_final=300, gate=GateConfig(mode="prefer")),
            "interactions": IntentPolicy(doc_topN=400, k_final=300, gate=GateConfig(mode="prefer")),
            "side_effects": IntentPolicy(doc_topN=400, k_final=300, gate=GateConfig(mode="prefer")),
            "unknown": IntentPolicy(doc_topN=400, k_final=300, gate=GateConfig(mode="prefer")),
        }
        if not path or not os.path.exists(path) or yaml is None:
            return default
        with open(path, "r", encoding="utf-8") as f:
            raw = yaml.safe_load(f) or {}
        out: Dict[str, IntentPolicy] = {}
        for key, val in raw.items():
            gate = val.get("gate", {})
            out[key] = IntentPolicy(
                doc_topN=int(val.get("doc_topN", default.get(key, default["default"]).doc_topN)),
                k_final=int(val.get("k_final", default.get(key, default["default"]).k_final)),
                gate=GateConfig(
                    mode=str(gate.get("mode", default.get(key, default["default"]).gate.mode)),
                    sections=list(gate.get("sections", default.get(key, default["default"]).gate.sections)),
                    weight=float(gate.get("weight", default.get(key, default["default"]).gate.weight)),
                ),
            )
        # гарантуємо наявність дефолтів
        for k, v in default.items():
            out.setdefault(k, v)
        return out


# ------------------- engine -------------------
@dataclass
class ParquetHybridEngine:
    index_dir: Path
    doc_index_dir: Path
    embed_model_name: str
    ce_model_name: str
    rrf_alpha: float = 60.0
    use_rewrite: bool = False
    aliases_csv: Optional[Path] = None
    rewrite_max_terms: int = 5
    intent_policy_map: Dict[str, IntentPolicy] = field(default_factory=dict)
    dump_eval_dir: Optional[Path] = None
    priors_jsonl: Optional[Path] = None

    # runtime
    encoder: Any = field(init=False, default=None)
    faiss_index: Any = field(init=False, default=None)
    doc_prefilter: Any = field(init=False, default=None)
    chunks_df: Optional[pd.DataFrame] = field(init=False, default=None)
    ce: Any = field(init=False, default=None)
    expander: Optional[AliasExpander] = field(init=False, default=None)
    priors: Optional[ClinicalPriors] = field(init=False, default=None)

    def __post_init__(self):
        # load vectors / indices
        if faiss is None:
            raise RuntimeError("faiss is not available")
        faiss_path = self.index_dir / "faiss.index"
        if not faiss_path.exists():
            raise FileNotFoundError(f"FAISS index not found: {faiss_path}")
        self.faiss_index = faiss.read_index(str(faiss_path))
        print(f"[INFO] FAISS loaded: faiss.index | dim={self.faiss_index.d}")

        # query encoder
        if SentenceTransformer is None:
            raise RuntimeError("sentence-transformers is required")
        self.encoder = SentenceTransformer(self.embed_model_name)
        print(f"[INFO] Query encoder ready: {self.embed_model_name}")

        # doc prefilter (опційно)
        doc_index_path = self.doc_index_dir / "doc.index"
        if doc_index_path.exists():
            try:
                self.doc_prefilter = faiss.read_index(str(doc_index_path))
                print(f"[INFO] Doc prefilter ready: n=?, dim={self.doc_prefilter.d}")
            except Exception:
                self.doc_prefilter = None
        else:
            print("[INFO] Doc prefilter not found (optional).")

        # chunks parquet (щоб мати метадані / тексти, а також правильну колонку id)
        chunks_parquet = self.index_dir / "chunks.parquet"
        if not chunks_parquet.exists():
            raise FileNotFoundError(f"Parquet not found: {chunks_parquet}")
        table = pq.read_table(str(chunks_parquet), memory_map=True)
        self.chunks_df = table.to_pandas()
        # стандартизуємо id колонку
        if "id" not in self.chunks_df.columns:
            alt = _first_existing_column(self.chunks_df, ["chunk_id", "pid", "row_id"])
            if alt:
                self.chunks_df = self.chunks_df.rename(columns={alt: "id"})
        if "id" not in self.chunks_df.columns:
            # якщо зовсім немає - генеруємо
            self.chunks_df["id"] = np.arange(len(self.chunks_df))
        # уніфікуємо тип
        self.chunks_df["id"] = self.chunks_df["id"].astype(int)

        # cross-encoder
        if FlagReranker is None:
            raise RuntimeError("FlagEmbedding (FlagReranker) is required for CE")
        cm = (self.ce_model_name or '').strip().lower()
        if not cm or cm in ('none','off','0'):
            self.ce = None
            print('[INFO] CrossEncoder disabled')
        else:
            self.ce = FlagReranker(self.ce_model_name, use_fp16=True)
            print('[INFO] CrossEncoder active:', self.ce_model_name)

        print(f"[INFO] CrossEncoder active: {self.ce_model_name}")

        # alias expander
        if self.use_rewrite and self.aliases_csv and self.aliases_csv.exists():
            self.expander = AliasExpander(str(self.aliases_csv))
        else:
            self.expander = None

        # priors
        if self.priors_jsonl and ClinicalPriors is not None and Path(self.priors_jsonl).exists():
            self.priors = ClinicalPriors(str(self.priors_jsonl), allow_intents=("indication", "unknown"))
            print(f"[INFO] Clinical priors loaded: {len(self.priors.entries)}")
        else:
            self.priors = None

    # ---- retrieval helpers ----
    def _embed(self, texts: List[str]) -> np.ndarray:
        return np.asarray(self.encoder.encode(texts, batch_size=32, convert_to_numpy=True, show_progress_bar=False))

    def _faiss_topk(self, q_vec: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
        # q_vec: (1, dim)
        D, I = self.faiss_index.search(q_vec, k)
        return D[0], I[0]

    def _fetch_chunks(self, ids: List[int]) -> pd.DataFrame:
        if not ids:
            return self.chunks_df.iloc[0:0].copy()
        df = self.chunks_df
        sub = df[df["id"].isin(ids)].copy()
        return sub

    def _apply_ce(self, query: str, cand_df: pd.DataFrame, top_k: int = 200) -> Tuple[pd.DataFrame, Dict[int, float]]:
        if cand_df.empty:
            return cand_df, {}
        # готуємо пари
        texts = cand_df.get("text")
        if texts is None:
            text_col = _first_existing_column(cand_df, ["text", "chunk_text", "body"])
            if text_col is None:
                # нічого не можемо зробити
                return cand_df, {}
            texts = cand_df[text_col]

        pairs = [[query, t] for t in texts.tolist()]
        scores = self.ce.compute_score(pairs, normalize=True)  # already in [0,1]
        cand_df = cand_df.copy()
        cand_df["ce_norm"] = scores
        # відсортуємо по ce
        cand_df = cand_df.sort_values("ce_norm", ascending=False)
        # повернемо до top_k
        cand_df = cand_df.head(min(len(cand_df), top_k)).reset_index(drop=True)
        ce_map = {int(row["id"]): float(row["ce_norm"]) for _, row in cand_df.iterrows()}
        return cand_df, ce_map

    # ---- main search once ----
    def search_once(
        self,
        q_raw: str,
        intent: str,
        policy: IntentPolicy,
        rrf_alpha: float,
        rewrite_max_terms: int = 5,
        dump_dir: Optional[Path] = None,
        q_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        t0 = time.time()
        trace: Dict[str, Any] = {"q_raw": q_raw, "intent": intent}
        # 1) normalize
        q_norm = normalize_query(q_raw)
        trace["q_norm"] = q_norm

        # 2) rewrite (aliases)
        q_variants: List[str] = [q_norm]
        if self.use_rewrite and self.expander is not None:
            try:
                expansions = self.expander.expand(q_norm, max_terms=rewrite_max_terms) or []
                if expansions:
                    q_variants = [q_norm] + expansions
            except Exception:
                pass
        trace["q_variants"] = q_variants

        # 3) clinical priors (дозволяємо для indication та unknown)
        prior_hit = None
        prior_terms: List[str] = []
        if self.priors is not None and intent in ("indication", "unknown"):
            try:
                prior_hit = self.priors.match(q_norm, intent)
                if prior_hit:
                    prior_terms = list(dict.fromkeys(prior_hit.get("terms", [])))[:3]
                    if prior_terms:
                        # терми додаємо в перший BM25-варіант (тут це перший текстовий варіант запиту)
                        q_variants[0] = (q_variants[0] + " " + " ".join(prior_terms)).strip()
            except Exception:
                prior_hit = None
                prior_terms = []
        trace["priors"] = {
            "enabled": self.priors is not None,
            "matched_prior": bool(prior_hit),
            "prior_terms": prior_terms,
            "prior_id": prior_hit.get("matched_id") if prior_hit else None,
            "matched_trigger": prior_hit.get("matched_trigger") if prior_hit else None,
        }

        # 4) dense retrieval (через FAISS) по першому варіанту (як основному)
        q_for_dense = q_variants[0] if q_variants else q_norm
        q_vec = self._embed([q_for_dense])
        # беремо doc_topN як верхню межу кандидатів
        doc_topN = int(policy.doc_topN)
        D, I = self._faiss_topk(q_vec, k=doc_topN)
        cand_ids = [int(i) for i in I.tolist() if i >= 0]
        trace["faiss"] = {"topN": doc_topN, "cand_count": len(cand_ids)}

        # 5) забираємо кандидати
        cand_df = self._fetch_chunks(cand_ids)

        # 6) CE rerank (без додаткових ваг — використовуємо нормовані ce_norm)
        ce_top = min(200, policy.k_final)
        cand_df, ce_map = self._apply_ce(q_for_dense, cand_df, top_k=ce_top)
        trace["ce"] = {"used": True, "top": ce_top, "scored": len(cand_df)}

        # 7) фінальний зріз k_final
        final_k = min(policy.k_final, len(cand_df))
        final_df = cand_df.head(final_k).reset_index(drop=True)
        results = [{"id": int(r["id"]), "score": float(r.get("ce_norm", 0.0))} for _, r in final_df.iterrows()]

        elapsed = time.time() - t0

        out = {
            "query": q_raw,
            "intent": intent,
            "q_norm": q_norm,
            "q_effective": q_for_dense,
            "k": int(policy.k_final),
            "doc_topN": int(policy.doc_topN),
            "results": results,
            "elapsed_sec": elapsed,
            "trace": trace,
        }

        # 8) dump у runs (один файл на запит)
        if dump_dir:
            _ensure_dir(dump_dir)
            fname = f"{q_id or 'q'}{'' if q_id and q_id.startswith('q') else ''}.json"
            # якщо q_id вигляду "q0007" — файлик буде "q0007.json"
            out_path = dump_dir / (f"{q_id}.json" if q_id else f"q.json")
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(out, f, ensure_ascii=False, indent=2, default=_json_default)

        return out


# ------------------- metrics & eval -------------------
def precision_at_k(pred_ids: List[int], gold_ids: List[int], k: int) -> float:
    if k <= 0:
        return 0.0
    pred = pred_ids[:k]
    if not pred:
        return 0.0
    inter = len(set(pred) & set(gold_ids))
    return inter / float(k)


def recall_at_k(pred_ids: List[int], gold_ids: List[int], k: int) -> float:
    if not gold_ids:
        return 0.0
    pred = pred_ids[:k]
    inter = len(set(pred) & set(gold_ids))
    return inter / float(len(gold_ids))


def ndcg_at_k(pred_ids: List[int], gold_ids: List[int], k: int) -> float:
    if not gold_ids:
        return 0.0
    pred = pred_ids[:k]
    dcg = 0.0
    for i, pid in enumerate(pred, start=1):
        if pid in gold_ids:
            dcg += 1.0 / math.log2(i + 1)
    # ideal
    ideal_hits = min(len(gold_ids), k)
    idcg = sum(1.0 / math.log2(i + 1) for i in range(1, ideal_hits + 1))
    return dcg / idcg if idcg > 0 else 0.0


def evaluate(
    engine: ParquetHybridEngine,
    queries_path: Path,
    policy_map: Dict[str, IntentPolicy],
    rrf_alpha: float,
    dump_eval_dir: Optional[Path] = None,
    subset_tag: str = "full",
) -> Dict[str, Any]:
    # читаємо JSONL
    data: List[Dict[str, Any]] = []
    with open(queries_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            data.append(json.loads(line))

    print(f"[INFO] Loaded queries: {len(data)}")

    # каталоги логів
    run_dir: Optional[Path] = None
    if dump_eval_dir:
        run_dir = dump_eval_dir / f"run_{_now_ts()}"
        _ensure_dir(run_dir)

    # прогін
    all_prec10: List[float] = []
    all_recall300: List[float] = []
    all_ndcg300: List[float] = []

    for idx, item in enumerate(data):
        q = item.get("query", "")
        intent = item.get("intent", "unknown")
        gold_ids = _safe_to_list(item.get("gold_doc_ids", []))

        policy = policy_map.get(intent, policy_map.get("default", IntentPolicy()))
        qid = f"q{idx:04d}"

        out = engine.search_once(
            q_raw=q,
            intent=intent,
            policy=policy,
            rrf_alpha=rrf_alpha,
            rewrite_max_terms=engine.rewrite_max_terms,
            dump_dir=run_dir,
            q_id=qid,
        )

        preds = [int(r["id"]) for r in out.get("results", [])]

        p10 = precision_at_k(preds, gold_ids, 10)
        r300 = recall_at_k(preds, gold_ids, 300)
        n300 = ndcg_at_k(preds, gold_ids, 300)

        all_prec10.append(p10)
        all_recall300.append(r300)
        all_ndcg300.append(n300)

    # агрегат
    P10 = float(np.mean(all_prec10)) if all_prec10 else 0.0
    R300 = float(np.mean(all_recall300)) if all_recall300 else 0.0
    NDCG300 = float(np.mean(all_ndcg300)) if all_ndcg300 else 0.0

    print("\n=== METRICS ===")
    print(f"Precision@10: {P10:.4f}")
    print(f"Recall@300: {R300:.4f}")
    print(f"nDCG@300: {NDCG300:.4f}")
    print(f"N: {len(data)}")
    print(f"subset: {subset_tag}")

    if run_dir:
        with open(run_dir / "metrics.json", "w", encoding="utf-8") as f:
            json.dump(
                {
                    "subset": subset_tag,
                    "N": len(data),
                    "Precision@10": P10,
                    "Recall@300": R300,
                    "nDCG@300": NDCG300,
                },
                f,
                ensure_ascii=False,
                indent=2,
                default=_json_default,
            )

    return {"P10": P10, "R300": R300, "NDCG300": NDCG300, "N": len(data), "subset": subset_tag}


# ------------------- cli -------------------
def build_argparser() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        "Assistant from Parquet — hybrid FAISS + CE (+ rewrite + priors) eval",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    p.add_argument("--index_dir", required=True, type=Path)
    p.add_argument("--doc_index_dir", required=True, type=Path)
    p.add_argument("--embed_model", default="intfloat/multilingual-e5-base", type=str)
    p.add_argument("--ce_model", default="BAAI/bge-reranker-v2-m3", type=str)
    p.add_argument("--queries", required=True, type=Path)

    p.add_argument("--intent_policy", type=Path, default=None)
    p.add_argument("--rrf_alpha", type=float, default=60.0)

    # rewrite
    p.add_argument("--use_rewrite", action="store_true")
    p.add_argument("--rewrite_aliases_csv", type=Path, default=None)
    p.add_argument("--rewrite_max_terms", type=int, default=5)

    # priors
    p.add_argument("--priors_jsonl", type=Path, default=None)

    # dumps
    p.add_argument("--dump_eval_dir", type=Path, default=None)

    return p.parse_args()


def main():
    args = build_argparser()

    policy_map = IntentPolicy.from_yaml(str(args.intent_policy) if args.intent_policy else None)

    engine = ParquetHybridEngine(
        index_dir=args.index_dir,
        doc_index_dir=args.doc_index_dir,
        embed_model_name=args.embed_model,
        ce_model_name=args.ce_model,
        rrf_alpha=args.rrf_alpha,
        use_rewrite=bool(args.use_rewrite),
        aliases_csv=args.rewrite_aliases_csv,
        rewrite_max_terms=int(args.rewrite_max_terms),
        intent_policy_map=policy_map,
        dump_eval_dir=args.dump_eval_dir,
        priors_jsonl=args.priors_jsonl,
    )

    subset = "full"
    metrics = evaluate(
        engine=engine,
        queries_path=args.queries,
        policy_map=policy_map,
        rrf_alpha=args.rrf_alpha,
        dump_eval_dir=args.dump_eval_dir,
        subset_tag=subset,
    )


if __name__ == "__main__":
    main()
