#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced Hybrid Medical Search Engine
BM25 + Vector Search + RRF + CrossEncoder –¥–ª—è –º–µ–¥–∏—á–Ω–æ–≥–æ –ø–æ—à—É–∫—É
"""

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, CrossEncoder
from sentence_transformers.util import cos_sim
from rank_bm25 import BM25Okapi
from typing import List, Dict, Tuple, Optional
import re
import argparse
from pathlib import Path
import json
import time
from collections import defaultdict

class AdvancedMedicalSearchEngine:
    """
    Hybrid search –∑ BM25 + Vector + RRF + CrossEncoder
    """
    
    def __init__(self, model_path: str, data_path: str, crossencoder_model: str = None):
        print("üîÑ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Advanced Medical Search Engine...")
        
        # Load models
        self.sentence_model = SentenceTransformer(model_path)
        self.crossencoder = CrossEncoder(
            crossencoder_model or 'cross-encoder/ms-marco-MiniLM-L-2-v2'
        ) if crossencoder_model != "none" else None
        
        # Load data
        self.df = pd.read_parquet(data_path)
        print(f"üìä –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.df)} –º–µ–¥–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")
        
        # Initialize search components
        self.documents = []
        self.doc_embeddings = None
        self.bm25_index = None
        self.contraindications_index = {}
        
        # Search statistics
        self.search_stats = {
            'total_searches': 0,
            'avg_bm25_time': 0,
            'avg_vector_time': 0,
            'avg_crossencoder_time': 0
        }
        
        self._prepare_search_indices()
    
    def _clean_text_for_bm25(self, text: str) -> List[str]:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç—É –¥–ª—è BM25 tokenization"""
        if not isinstance(text, str):
            return []
        
        # –í–∏–¥–∞–ª–∏—Ç–∏ [SEP] —Ç–æ–∫–µ–Ω–∏ —Ç–∞ –æ—á–∏—Å—Ç–∏—Ç–∏
        text = text.replace('[SEP]', ' ')
        # Lowercase —Ç–∞ –≤–∏–¥–∞–ª–∏—Ç–∏ –∑–∞–π–≤—ñ —Å–∏–º–≤–æ–ª–∏
        text = re.sub(r'[^\w\s\-]', ' ', text.lower())
        # Tokenize
        tokens = text.split()
        # –í–∏–¥–∞–ª–∏—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫—ñ —Ç–æ–∫–µ–Ω–∏
        tokens = [token for token in tokens if len(token) > 2]
        
        return tokens
    
    def _prepare_search_indices(self):
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å—ñ—Ö —ñ–Ω–¥–µ–∫—Å—ñ–≤ –¥–ª—è –ø–æ—à—É–∫—É"""
        print("üèóÔ∏è  –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —ñ–Ω–¥–µ–∫—Å—ñ–≤...")
        
        bm25_docs = []
        
        for idx, row in self.df.iterrows():
            # –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –ø–æ—à—É–∫—É
            doc_parts = []
            contraindications = ""
            
            if pd.notna(row['–ù–∞–∑–≤–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—É']):
                doc_parts.append(str(row['–ù–∞–∑–≤–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—É']))
            
            if '–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞' in row and pd.notna(row['–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞']):
                group_text = str(row['–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞'])[:200]
                doc_parts.append(group_text)
            
            if pd.notna(row['–ü–æ–∫–∞–∑–∞–Ω–Ω—è']):
                indications = str(row['–ü–æ–∫–∞–∑–∞–Ω–Ω—è'])[:800]
                doc_parts.append(indications)
            
            if '–ü—Ä–æ—Ç–∏–ø–æ–∫–∞–∑–∞–Ω–Ω—è' in row and pd.notna(row['–ü—Ä–æ—Ç–∏–ø–æ–∫–∞–∑–∞–Ω–Ω—è']):
                contraindications = str(row['–ü—Ä–æ—Ç–∏–ø–æ–∫–∞–∑–∞–Ω–Ω—è'])[:500]
            
            # –î–æ–∫—É–º–µ–Ω—Ç –¥–ª—è vector search
            doc_text = ' [SEP] '.join(doc_parts)
            
            # –î–æ–∫—É–º–µ–Ω—Ç –¥–ª—è BM25 (–±–µ–∑ [SEP], –±—ñ–ª—å—à –ø—Ä–∏—Ä–æ–¥–Ω–∏–π)
            bm25_text = ' '.join(doc_parts)
            bm25_tokens = self._clean_text_for_bm25(bm25_text)
            
            self.documents.append({
                'id': idx,
                'text': doc_text,
                'bm25_text': bm25_text,
                'name': row['–ù–∞–∑–≤–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—É'],
                'indications': row['–ü–æ–∫–∞–∑–∞–Ω–Ω—è'] if pd.notna(row['–ü–æ–∫–∞–∑–∞–Ω–Ω—è']) else "",
                'contraindications': contraindications,
                'therapeutic_group': row.get('–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞', ""),
                'original_row': row
            })
            
            bm25_docs.append(bm25_tokens)
            self.contraindications_index[idx] = contraindications.lower()
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ BM25 —ñ–Ω–¥–µ–∫—Å
        print("üîç –°—Ç–≤–æ—Ä–µ–Ω–Ω—è BM25 —ñ–Ω–¥–µ–∫—Å—É...")
        self.bm25_index = BM25Okapi(bm25_docs)
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ vector —ñ–Ω–¥–µ–∫—Å
        print("üß† –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è embeddings...")
        doc_texts = [doc['text'] for doc in self.documents]
        self.doc_embeddings = self.sentence_model.encode(
            doc_texts, 
            show_progress_bar=True,
            batch_size=32
        )
        
        print("‚úÖ –í—Å—ñ —ñ–Ω–¥–µ–∫—Å–∏ –≥–æ—Ç–æ–≤—ñ!")
    
    def _bm25_search(self, query: str, top_k: int = 50) -> List[Tuple[int, float]]:
        """BM25 –ø–æ—à—É–∫"""
        start_time = time.time()
        
        query_tokens = self._clean_text_for_bm25(query)
        if not query_tokens:
            return []
        
        # BM25 scoring
        scores = self.bm25_index.get_scores(query_tokens)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ scores (0-1)
        if scores.max() > 0:
            scores = scores / scores.max()
        
        # –¢–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        top_indices = np.argsort(scores)[::-1][:top_k]
        results = [(int(idx), float(scores[idx])) for idx in top_indices if scores[idx] > 0]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        elapsed = time.time() - start_time
        self.search_stats['avg_bm25_time'] = (
            self.search_stats['avg_bm25_time'] * self.search_stats['total_searches'] + elapsed
        ) / (self.search_stats['total_searches'] + 1)
        
        return results
    
    def _vector_search(self, query: str, top_k: int = 50) -> List[Tuple[int, float]]:
        """Vector –ø–æ—à—É–∫"""
        start_time = time.time()
        
        # Encode query
        query_embedding = self.sentence_model.encode(query)
        
        # Compute similarities
        similarities = cos_sim(query_embedding, self.doc_embeddings)[0]
        
        # –¢–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        top_indices = similarities.argsort(descending=True)[:top_k]
        results = [(int(idx), float(similarities[idx])) for idx in top_indices]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        elapsed = time.time() - start_time
        self.search_stats['avg_vector_time'] = (
            self.search_stats['avg_vector_time'] * self.search_stats['total_searches'] + elapsed
        ) / (self.search_stats['total_searches'] + 1)
        
        return results
    
    def _reciprocal_rank_fusion(self, bm25_results: List[Tuple[int, float]], 
                              vector_results: List[Tuple[int, float]], 
                              k: int = 60) -> List[Tuple[int, float]]:
        """Reciprocal Rank Fusion –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤"""
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ —Ä–∞–Ω–∫—ñ–Ω–≥–∏
        bm25_ranks = {doc_id: rank + 1 for rank, (doc_id, score) in enumerate(bm25_results)}
        vector_ranks = {doc_id: rank + 1 for rank, (doc_id, score) in enumerate(vector_results)}
        
        # –í—Å—ñ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏
        all_docs = set(bm25_ranks.keys()) | set(vector_ranks.keys())
        
        # RRF scoring
        rrf_scores = {}
        for doc_id in all_docs:
            rrf_score = 0
            if doc_id in bm25_ranks:
                rrf_score += 1 / (k + bm25_ranks[doc_id])
            if doc_id in vector_ranks:
                rrf_score += 1 / (k + vector_ranks[doc_id])
            
            rrf_scores[doc_id] = rrf_score
        
        # –°–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ RRF score
        sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)
        
        return sorted_results
    
    def _crossencoder_rerank(self, query: str, candidate_results: List[Tuple[int, float]], 
                           top_k: int = 10) -> List[Tuple[int, float]]:
        """CrossEncoder re-ranking"""
        if not self.crossencoder or len(candidate_results) == 0:
            return candidate_results[:top_k]
        
        start_time = time.time()
        
        # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –ø–∞—Ä–∏ query-document
        query_doc_pairs = []
        doc_ids = []
        
        for doc_id, _ in candidate_results[:20]:  # –û–±–º–µ–∂–∏—Ç–∏ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
            doc_text = self.documents[doc_id]['bm25_text'][:500]  # Truncate for CrossEncoder
            query_doc_pairs.append([query, doc_text])
            doc_ids.append(doc_id)
        
        if not query_doc_pairs:
            return []
        
        # CrossEncoder scoring
        ce_scores = self.crossencoder.predict(query_doc_pairs)
        
        # –û–±'—î–¥–Ω–∞—Ç–∏ –∑ doc_ids —Ç–∞ —Å–æ—Ä—Ç—É–≤–∞—Ç–∏
        reranked = list(zip(doc_ids, ce_scores))
        reranked.sort(key=lambda x: x[1], reverse=True)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        elapsed = time.time() - start_time
        self.search_stats['avg_crossencoder_time'] = (
            self.search_stats['avg_crossencoder_time'] * self.search_stats['total_searches'] + elapsed
        ) / (self.search_stats['total_searches'] + 1)
        
        return reranked[:top_k]
    
    def _check_contraindications(self, query: str, doc_id: int) -> Dict[str, any]:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–æ—Ç–∏–ø–æ–∫–∞–∑–∞–Ω—å"""
        contraindications = self.contraindications_index.get(doc_id, "")
        
        # –ü—Ä–æ—Å—Ç—ñ heuristics –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω—å
        warning_keywords = [
            '–≤–∞–≥—ñ—Ç–Ω—ñ—Å—Ç—å', '–≥–æ–¥—É–≤–∞–Ω–Ω—è –≥—Ä—É–¥–¥—é', '–¥—ñ—Ç–∏', '–ø–µ—á—ñ–Ω–∫–æ–≤–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—ñ—Å—Ç—å',
            '–Ω–∏—Ä–∫–æ–≤–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—ñ—Å—Ç—å', '—Å–µ—Ä—Ü–µ–≤–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—ñ—Å—Ç—å', '–∞–ª–µ—Ä–≥—ñ—è'
        ]
        
        warnings = []
        for keyword in warning_keywords:
            if keyword in contraindications:
                warnings.append(keyword)
        
        return {
            'has_contraindications': len(contraindications) > 0,
            'warnings': warnings,
            'contraindications_text': contraindications[:200] + "..." if len(contraindications) > 200 else contraindications
        }
    
    def search(self, query: str, top_k: int = 5, use_crossencoder: bool = True,
               include_safety: bool = True) -> Dict:
        """–ì–æ–ª–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –ø–æ—à—É–∫—É"""
        
        search_start = time.time()
        self.search_stats['total_searches'] += 1
        
        print(f"üîç –ü–æ—à—É–∫: '{query}'")
        
        # Stage 1: BM25 Search
        print("   üìù BM25 –ø–æ—à—É–∫...")
        bm25_results = self._bm25_search(query, top_k=50)
        
        # Stage 2: Vector Search  
        print("   üß† Vector –ø–æ—à—É–∫...")
        vector_results = self._vector_search(query, top_k=50)
        
        # Stage 3: Reciprocal Rank Fusion
        print("   üîÑ RRF –æ–±'—î–¥–Ω–∞–Ω–Ω—è...")
        rrf_results = self._reciprocal_rank_fusion(bm25_results, vector_results)
        
        # Stage 4: CrossEncoder Re-ranking
        if use_crossencoder and self.crossencoder:
            print("   üéØ CrossEncoder re-ranking...")
            final_results = self._crossencoder_rerank(query, rrf_results, top_k=top_k)
        else:
            final_results = rrf_results[:top_k]
        
        # Stage 5: Prepare results with safety info
        search_results = []
        for doc_id, score in final_results:
            doc = self.documents[doc_id]
            
            result = {
                'rank': len(search_results) + 1,
                'drug_name': doc['name'],
                'score': float(score),
                'indications': doc['indications'][:300] + "..." if len(doc['indications']) > 300 else doc['indications'],
                'therapeutic_group': doc['therapeutic_group'][:100] if doc['therapeutic_group'] else ""
            }
            
            # –î–æ–¥–∞—Ç–∏ safety information
            if include_safety:
                safety_info = self._check_contraindications(query, doc_id)
                result['safety'] = safety_info
            
            search_results.append(result)
        
        total_time = time.time() - search_start
        
        return {
            'query': query,
            'results': search_results,
            'search_info': {
                'total_time': f"{total_time:.3f}s",
                'bm25_candidates': len(bm25_results),
                'vector_candidates': len(vector_results),
                'rrf_candidates': len(rrf_results),
                'final_results': len(final_results),
                'used_crossencoder': use_crossencoder and self.crossencoder is not None
            },
            'disclaimer': "‚ö†Ô∏è  –ó–∞–≤–∂–¥–∏ –∫–æ–Ω—Å—É–ª—å—Ç—É–π—Ç–µ—Å—å –∑ –ª—ñ–∫–∞—Ä–µ–º –ø–µ—Ä–µ–¥ –ø—Ä–∏–π–æ–º–æ–º –ª—ñ–∫—ñ–≤"
        }
    
    def get_search_statistics(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—à—É–∫—É"""
        return {
            'total_searches': self.search_stats['total_searches'],
            'avg_times': {
                'bm25': f"{self.search_stats['avg_bm25_time']:.4f}s",
                'vector': f"{self.search_stats['avg_vector_time']:.4f}s", 
                'crossencoder': f"{self.search_stats['avg_crossencoder_time']:.4f}s"
            },
            'index_info': {
                'total_documents': len(self.documents),
                'embedding_dimension': self.doc_embeddings.shape[1] if self.doc_embeddings is not None else 0,
                'has_crossencoder': self.crossencoder is not None
            }
        }

def print_search_results(search_response: Dict):
    """–ö—Ä–∞—Å–∏–≤–æ –≤–∏–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—à—É–∫—É"""
    results = search_response['results']
    search_info = search_response['search_info']
    
    print(f"\n{'='*60}")
    print(f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è: '{search_response['query']}'")
    print(f"‚è±Ô∏è  –ß–∞—Å –ø–æ—à—É–∫—É: {search_info['total_time']}")
    print(f"üìä –ö–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤: BM25={search_info['bm25_candidates']}, Vector={search_info['vector_candidates']}")
    print(f"üéØ CrossEncoder: {'–¢–∞–∫' if search_info['used_crossencoder'] else '–ù—ñ'}")
    print(f"{'='*60}")
    
    if not results:
        print("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return
    
    for result in results:
        print(f"\n{result['rank']}. üíä {result['drug_name']}")
        print(f"   üìà –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å: {result['score']:.3f}")
        print(f"   üè• –ü–æ–∫–∞–∑–∞–Ω–Ω—è: {result['indications']}")
        
        if result.get('therapeutic_group'):
            print(f"   üìã –ì—Ä—É–ø–∞: {result['therapeutic_group']}")
        
        # Safety warnings
        if 'safety' in result:
            safety = result['safety']
            if safety['warnings']:
                print(f"   ‚ö†Ô∏è  –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: {', '.join(safety['warnings'])}")
            if safety['contraindications_text']:
                print(f"   üö´ –ü—Ä–æ—Ç–∏–ø–æ–∫–∞–∑–∞–Ω–Ω—è: {safety['contraindications_text']}")
    
    print(f"\n{search_response['disclaimer']}")

def main():
    parser = argparse.ArgumentParser(description="Advanced Hybrid Medical Search")
    parser.add_argument("--model", default="models/medical-search-ua-full", 
                       help="Path to fine-tuned sentence transformer")
    parser.add_argument("--data", default="data/processed/clean_medical.parquet",
                       help="Path to medical data")
    parser.add_argument("--crossencoder", default="cross-encoder/ms-marco-MiniLM-L-2-v2",
                       help="CrossEncoder model (use 'none' to disable)")
    parser.add_argument("--stats", action="store_true", 
                       help="Show search statistics")
    
    args = parser.parse_args()
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø–æ—à—É–∫–æ–≤–∏–∫
    search_engine = AdvancedMedicalSearchEngine(
        model_path=args.model,
        data_path=args.data, 
        crossencoder_model=args.crossencoder
    )
    
    if args.stats:
        stats = search_engine.get_search_statistics()
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–®–£–ö–û–í–ò–ö–ê:")
        print(json.dumps(stats, indent=2, ensure_ascii=False))
        return
    
    # –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π –ø–æ—à—É–∫
    print("\nüè• ADVANCED MEDICAL SEARCH ENGINE")
    print("–í–≤–µ–¥—ñ—Ç—å 'exit' –¥–ª—è –≤–∏—Ö–æ–¥—É, 'stats' –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
    print("–ö–æ–º–∞–Ω–¥–∏: --no-ce (–±–µ–∑ CrossEncoder), --no-safety (–±–µ–∑ safety check)")
    
    while True:
        user_input = input("\nüîç –í–≤–µ–¥—ñ—Ç—å –∑–∞–ø–∏—Ç: ").strip()
        
        if user_input.lower() in ['exit', 'quit', '–≤–∏—Ö—ñ–¥']:
            break
        
        if user_input.lower() == 'stats':
            stats = search_engine.get_search_statistics()
            print(json.dumps(stats, indent=2, ensure_ascii=False))
            continue
        
        if not user_input:
            continue
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–∞–Ω–¥
        use_crossencoder = '--no-ce' not in user_input
        include_safety = '--no-safety' not in user_input
        query = user_input.replace('--no-ce', '').replace('--no-safety', '').strip()
        
        # –ü–æ—à—É–∫
        try:
            search_response = search_engine.search(
                query, 
                top_k=5,
                use_crossencoder=use_crossencoder,
                include_safety=include_safety
            )
            print_search_results(search_response)
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É: {e}")

if __name__ == "__main__":
    main()