"""
04_build_training_pairs.py

–°—Ç–≤–æ—Ä–µ–Ω–Ω—è training pairs –¥–ª—è Stage 1 fine-tuning E5:
  - query (–ø–∞—Ü—ñ—î–Ω—Ç—Å—å–∫–∏–π –∑–∞–ø–∏—Ç)
  - positive (—Ç–µ–∫—Å—Ç –ø—Ä–µ–ø–∞—Ä–∞—Ç—É)
  - hard_negatives (—Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç—ñ–≤ —ñ–Ω—à–∏—Ö –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤)

–û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ:
  - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ sampled –ø—Ä–µ–ø–∞—Ä–∞—Ç–∏:
      data/training/finetuning/compendium_sampled.parquet
  - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –∑–∞–ø–∏—Ç–∏:
      data/training/finetuning/queries_generated.jsonl
  - –î–æ–∫—É–º–µ–Ω—Ç–∏ —Ç–∞ –≥—Ä—É–ø–∏ –ø—Ä–∏–≤'—è–∑—É—é—Ç—å—Å—è –¥–æ drug_id.
  - Hard negatives:
      * —á–∞—Å—Ç–∏–Ω–∞ –∑ —Ç—ñ—î—ó –∂ —Ñ–∞—Ä–º–≥—Ä—É–ø–∏ (very hard),
      * —á–∞—Å—Ç–∏–Ω–∞ –∑ —ñ–Ω—à–∏—Ö –≥—Ä—É–ø (medium hard).
  - –Ñ tqdm progressbar —ñ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ resume:
      * —è–∫—â–æ training_pairs_stage1.jsonl —É–∂–µ —ñ—Å–Ω—É—î,
        –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ query –∑ —É–∂–µ –Ω–∞—è–≤–Ω–∏–º–∏ qid.
"""

from __future__ import annotations

import json
import random
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Any, Tuple, Set

import pandas as pd
from tqdm import tqdm

# -----------------------------
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ —Ç–∞ –∫–æ–Ω—Ñ—ñ–≥
# -----------------------------
RANDOM_SEED = 42
NUM_NEGATIVES = 5

# –ú–∞–∫—Å. –¥–æ–≤–∂–∏–Ω–∏ –ø–æ–ª—ñ–≤ —É –¥–æ–∫—É–º–µ–Ω—Ç—ñ (—Å–∏–º–≤–æ–ª–∏)
MAX_GROUP_CHARS = 200
MAX_INDICATION_CHARS = 600
MAX_COMPOSITION_CHARS = 200


def get_paths() -> Tuple[Path, Path, Path]:
    """
    –í–∏—Ä–∞—Ö–æ–≤—É—î–º–æ —à–ª—è—Ö–∏ –≤—ñ–¥–Ω–æ—Å–Ω–æ –∫–æ—Ä–µ–Ω—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—é.
    """
    script_path = Path(__file__).resolve()
    repo_root = script_path.parents[2]

    queries_path = (
        repo_root / "data" / "training" / "finetuning" / "queries_generated.jsonl"
    )
    compendium_path = (
        repo_root / "data" / "training" / "finetuning" / "compendium_sampled.parquet"
    )
    output_path = (
        repo_root / "data" / "training" / "finetuning" / "training_pairs_stage1.jsonl"
    )

    return queries_path, compendium_path, output_path


# -----------------------------
# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞
# -----------------------------
def load_queries(queries_path: Path) -> List[Dict[str, Any]]:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –∑–∞–ø–∏—Ç–∏.

    –§—ñ–ª—å—Ç—Ä—É—î–º–æ:
      - —Ç—ñ–ª—å–∫–∏ –∑–∞–ø–∏—Å–∏ –∑ num_queries > 0,
      - –ø—É—Å—Ç—ñ / –ø—Ä–æ–±—ñ–ª—å–Ω—ñ —Ä—è–¥–∫–∏,
      - –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑–∞–ø–∏—Ç—ñ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç—É.

    –î–æ–¥–∞—î–º–æ –ø–æ–ª–µ qid ‚Äî –≥–ª–æ–±–∞–ª—å–Ω–∏–π —ñ–Ω–¥–µ–∫—Å query, —è–∫–∏–π –±—É–¥–µ–º–æ
    –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è resume.
    """
    print(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ queries –∑: {queries_path}")

    raw_queries: List[Dict[str, Any]] = []

    with queries_path.open("r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            data = json.loads(line)

            if data.get("num_queries", 0) <= 0:
                continue

            drug_id = int(data["drug_id"])
            drug_name = str(data.get("drug_name", "")).strip()

            seen_for_drug: Set[str] = set()
            for q in data.get("queries", []):
                q = (q or "").strip()
                if not q:
                    continue
                if q in seen_for_drug:
                    continue
                seen_for_drug.add(q)

                raw_queries.append(
                    {
                        "query": q,
                        "drug_id": drug_id,
                        "drug_name": drug_name,
                    }
                )

    # –ü—Ä–∏—Å–≤–æ—é—î–º–æ qid
    queries_with_id: List[Dict[str, Any]] = []
    for qid, item in enumerate(raw_queries):
        item = dict(item)
        item["qid"] = qid
        queries_with_id.append(item)

    print(f"   ‚úÖ –í–∞–ª—ñ–¥–Ω–∏—Ö query-—Ä—è–¥–∫—ñ–≤: {len(queries_with_id):,}")
    return queries_with_id


def prepare_compendium(compendium_path: Path) -> pd.DataFrame:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ compendium_sampled.parquet —ñ –≥–∞—Ä–∞–Ω—Ç—É—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∏ drug_id.
    –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ñ–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω—É –≥—Ä—É–ø—É.
    """
    print(f"\nüìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ sampled compendium: {compendium_path}")
    df = pd.read_parquet(compendium_path)

    # –Ø–∫—â–æ –Ω–µ–º–∞ drug_id ‚Äì —Å—Ç–≤–æ—Ä—é—î–º–æ –∑ —ñ–Ω–¥–µ–∫—Å—É (—è–∫ —É —Å–∫—Ä–∏–ø—Ç—ñ 03)
    if "drug_id" not in df.columns:
        df = df.reset_index(drop=True)
        df["drug_id"] = df.index
    else:
        df = df.reset_index(drop=True)

    print(f"   –ü—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤ —É sampled –Ω–∞–±–æ—Ä—ñ: {len(df):,}")

    # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –≥—Ä—É–ø
    df["group"] = (
        df["–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞"]
        .fillna("")
        .astype(str)
        .str.strip()
        .replace("", "Unknown")
    )

    return df


def create_drug_documents(df: pd.DataFrame) -> Dict[int, str]:
    """
    –°—Ç–≤–æ—Ä—é—î —Ç–µ–∫—Å—Ç–æ–≤–∏–π document –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç—É.
    –ö–ª—é—á ‚Äì drug_id.
    """
    print("\nüìù –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö document'—ñ–≤ –¥–ª—è –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤...")

    documents: Dict[int, str] = {}

    for _, row in tqdm(df.iterrows(), total=len(df), desc="Building documents"):
        drug_id = int(row["drug_id"])

        parts: List[str] = []

        # –ù–∞–∑–≤–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—É
        name = str(row.get("–ù–∞–∑–≤–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—É", "")).strip()
        if name:
            parts.append(name)

        # –§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞ (–æ–±—Ä—ñ–∑–∞—î–º–æ)
        group = str(row.get("–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞", "")).strip()
        if group:
            parts.append(group[:MAX_GROUP_CHARS])

        # –ü–æ–∫–∞–∑–∞–Ω–Ω—è (–æ—Å–Ω–æ–≤–Ω–µ –ø–æ–ª–µ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏)
        indications = str(row.get("–ü–æ–∫–∞–∑–∞–Ω–Ω—è", "")).strip()
        if indications:
            parts.append(indications[:MAX_INDICATION_CHARS])

        # –°–∫–ª–∞–¥ (–∫–æ—Ä–æ—Ç–∫–æ)
        composition = str(row.get("–°–∫–ª–∞–¥", "")).strip()
        if composition:
            parts.append(composition[:MAX_COMPOSITION_CHARS])

        doc_text = " | ".join(parts).strip()
        if not doc_text:
            continue

        documents[drug_id] = doc_text

    print(f"   ‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ document'—ñ–≤: {len(documents):,}")
    return documents


def build_group_indices(
    df: pd.DataFrame,
) -> Tuple[Dict[str, List[int]], Dict[int, str]]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î:
      - groups_map: group -> [drug_id, ...]
      - id_to_group: drug_id -> group
    """
    print("\nüìä –ü–æ–±—É–¥–æ–≤–∞ —ñ–Ω–¥–µ–∫—Å—ñ–≤ –≥—Ä—É–ø...")

    groups_map: Dict[str, List[int]] = defaultdict(list)
    id_to_group: Dict[int, str] = {}

    for _, row in tqdm(df.iterrows(), total=len(df), desc="Indexing groups"):
        drug_id = int(row["drug_id"])
        group = str(row["group"])
        groups_map[group].append(drug_id)
        id_to_group[drug_id] = group

    print(f"   –ì—Ä—É–ø –ø–æ–∫—Ä–∏—Ç–æ: {len(groups_map):,}")
    return groups_map, id_to_group


# -----------------------------
# Resume: —á–∏—Ç–∞–Ω–Ω—è –≤–∂–µ –≥–æ—Ç–æ–≤–∏—Ö qid
# -----------------------------
def load_processed_qids(output_path: Path) -> Set[int]:
    """
    –ß–∏—Ç–∞—î —ñ—Å–Ω—É—é—á–∏–π —Ñ–∞–π–ª training_pairs_stage1.jsonl —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –º–Ω–æ–∂–∏–Ω—É qid,
    —è–∫—ñ –≤–∂–µ –±—É–ª–∏ –æ–±—Ä–æ–±–ª–µ–Ω—ñ.

    –Ø–∫—â–æ qid –≤—ñ–¥—Å—É—Ç–Ω—ñ–π —É metadata (—Å—Ç–∞—Ä–∏–π —Ñ–æ—Ä–º–∞—Ç) ‚Äì –ø—Ä–æ—Å—Ç–æ —ñ–≥–Ω–æ—Ä—É—î–º–æ —Ç–∞ –Ω–µ
    –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ –¥–ª—è resume.
    """
    processed: Set[int] = set()

    if not output_path.exists():
        return processed

    print(f"\nüìÑ –ó–Ω–∞–π–¥–µ–Ω–æ —ñ—Å–Ω—É—é—á–∏–π —Ñ–∞–π–ª training pairs: {output_path}")
    print("   –ß–∏—Ç–∞—î–º–æ –≤–∂–µ –æ–±—Ä–æ–±–ª–µ–Ω—ñ qid –¥–ª—è resume...")

    with output_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except Exception:
                continue
            meta = obj.get("metadata", {})
            qid = meta.get("qid")
            if isinstance(qid, int):
                processed.add(qid)

    print(f"   –í–∂–µ –æ–ø—Ä–∞—Ü—å–æ–≤–∞–Ω–æ query –∑ qid: {len(processed):,}")
    return processed


# -----------------------------
# Hard negatives mining + streaming –∑–∞–ø–∏—Å
# -----------------------------
def mine_hard_negatives_and_write(
    queries_data: List[Dict[str, Any]],
    documents: Dict[int, str],
    groups_map: Dict[str, List[int]],
    id_to_group: Dict[int, str],
    output_path: Path,
    processed_qids: Set[int],
    num_negatives: int = NUM_NEGATIVES,
) -> Tuple[int, float, Dict[str, Any]]:
    """
    –°—Ç–≤–æ—Ä—é—î–º–æ training pairs –∑ hard negatives —ñ –æ–¥—Ä–∞–∑—É –∑–∞–ø–∏—Å—É—î–º–æ —ó—Ö —É —Ñ–∞–π–ª
    (streaming).

    –°—Ç—Ä–∞—Ç–µ–≥—ñ—è:
      - 0‚Äì3 –Ω–µ–≥–∞—Ç–∏–≤–∏ –∑ —Ç—ñ—î—ó –∂ –≥—Ä—É–ø–∏ (—è–∫—â–æ —î –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤),
      - —Ä–µ—à—Ç–∞ ‚Äì –≤–∏–ø–∞–¥–∫–æ–≤—ñ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∏ –∑ —ñ–Ω—à–∏—Ö –≥—Ä—É–ø.

    –ü–æ–≤–µ—Ä—Ç–∞—î:
      - total_pairs: —Å–∫—ñ–ª—å–∫–∏ –ø–∞—Ä —Å—Ç–≤–æ—Ä–µ–Ω–æ –≤ —Ü—ñ–π —Å–µ—Å—ñ—ó,
      - avg_negs: —Å–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–µ–≥–∞—Ç–∏–≤—ñ–≤,
      - sample_pair: –æ–¥–Ω–∞ –∑ –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –ø–∞—Ä (–¥–ª—è –ª–æ–≥—ñ–≤).
    """
    print("\nüî® Mining hard negatives —Ç–∞ –∑–∞–ø–∏—Å —É —Ñ–∞–π–ª...")

    random.seed(RANDOM_SEED)

    all_drug_ids = list(documents.keys())
    total_pairs = 0
    total_negs = 0
    sample_pair: Dict[str, Any] | None = None

    # –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ñ–∞–π–ª —É —Ä–µ–∂–∏–º—ñ –¥–æ–ø–∏—Å—É–≤–∞–Ω–Ω—è (append)
    mode = "a" if output_path.exists() else "w"
    with output_path.open(mode, encoding="utf-8") as f_out:
        iterator = tqdm(
            queries_data,
            desc="Building pairs",
            total=len(queries_data),
        )

        for item in iterator:
            qid = int(item["qid"])
            if qid in processed_qids:
                # –£–∂–µ —î –≤ —Ñ–∞–π–ª—ñ ‚Äì –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ
                continue

            query = item["query"]
            pos_drug_id = int(item["drug_id"])
            drug_name = item["drug_name"]

            if pos_drug_id not in documents:
                continue

            positive_doc = documents[pos_drug_id]
            drug_group = id_to_group.get(pos_drug_id, "Unknown")

            neg_ids: List[int] = []

            # --- 1) Very hard negatives: –∑ —Ç—ñ—î—ó –∂ –≥—Ä—É–ø–∏ ---
            same_group_ids = [
                d for d in groups_map.get(drug_group, []) if d != pos_drug_id
            ]
            if same_group_ids:
                n_same = min(3, num_negatives, len(same_group_ids))
                neg_ids.extend(random.sample(same_group_ids, n_same))

            # --- 2) Medium hard: –≤–∏–ø–∞–¥–∫–æ–≤—ñ –∑ —ñ–Ω—à–∏—Ö –≥—Ä—É–ø ---
            max_candidates = len(all_drug_ids) - 1  # –æ–∫—Ä—ñ–º positive
            while len(neg_ids) < num_negatives and len(neg_ids) < max_candidates:
                candidate = random.choice(all_drug_ids)
                if candidate == pos_drug_id or candidate in neg_ids:
                    continue
                neg_ids.append(candidate)

            negative_docs = [documents[d_id] for d_id in neg_ids if d_id in documents]
            if not negative_docs:
                continue

            pair: Dict[str, Any] = {
                "query": query,
                "positive": positive_doc,
                "hard_negatives": negative_docs,
                "metadata": {
                    "qid": qid,
                    "drug_id": pos_drug_id,
                    "drug_name": drug_name,
                    "therapeutic_group": drug_group,
                    "num_negatives": len(negative_docs),
                },
            }

            # –ó–∞–ø–∏—Å—É—î–º–æ –≤ —Ñ–∞–π–ª
            f_out.write(json.dumps(pair, ensure_ascii=False) + "\n")

            # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            total_pairs += 1
            total_negs += len(negative_docs)
            if sample_pair is None:
                sample_pair = pair

    if total_pairs == 0:
        return 0, 0.0, {}

    avg_negs = total_negs / total_pairs
    return total_pairs, avg_negs, sample_pair or {}


# -----------------------------
# main
# -----------------------------
def main():
    print("=" * 80)
    print("üî® –°–¢–í–û–†–ï–ù–ù–Ø TRAINING PAIRS –ó HARD NEGATIVES (Stage 1)")
    print("=" * 80)

    queries_path, compendium_path, output_path = get_paths()

    # 1) –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ñ–∞–π–ª—ñ–≤
    if not queries_path.exists():
        raise FileNotFoundError(f"Queries file –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {queries_path}")
    if not compendium_path.exists():
        raise FileNotFoundError(f"Compendium file –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {compendium_path}")

    # 2) –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    df = prepare_compendium(compendium_path)
    queries_data = load_queries(queries_path)

    # 3) –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —Ç–∞ –≥—Ä—É–ø–æ–≤–∏—Ö —ñ–Ω–¥–µ–∫—Å—ñ–≤
    documents = create_drug_documents(df)
    groups_map, id_to_group = build_group_indices(df)

    # 4) Resume: —á–∏—Ç–∞—î–º–æ –≤–∂–µ –æ–ø—Ä–∞—Ü—å–æ–≤–∞–Ω—ñ qid
    processed_qids = load_processed_qids(output_path)

    # 5) Mining + –∑–∞–ø–∏—Å —É —Ñ–∞–π–ª
    total_pairs_new, avg_negs, sample_pair = mine_hard_negatives_and_write(
        queries_data=queries_data,
        documents=documents,
        groups_map=groups_map,
        id_to_group=id_to_group,
        output_path=output_path,
        processed_qids=processed_qids,
        num_negatives=NUM_NEGATIVES,
    )

    # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä (—Å—Ç–∞—Ä—ñ + –Ω–æ–≤—ñ)
    total_pairs_total = len(processed_qids) + total_pairs_new

    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –ù–æ–≤–∏—Ö training pairs —É —Ü—ñ–π —Å–µ—Å—ñ—ó: {total_pairs_new:,}")
    print(f"   –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö): {total_pairs_total:,}")
    if total_pairs_new > 0:
        print(f"   –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–µ–≥–∞—Ç–∏–≤—ñ–≤ –Ω–∞ query (—É –Ω–æ–≤–∏—Ö –ø–∞—Ä–∞—Ö): {avg_negs:.2f}")

    if total_pairs_new > 0 and sample_pair:
        print("\nüìù –ü—Ä–∏–∫–ª–∞–¥ training pair (–∑ —Ü—ñ—î—ó —Å–µ—Å—ñ—ó):")
        print(f"   Query:    {sample_pair['query'][:120]}...")
        print(f"   Positive: {sample_pair['positive'][:120]}...")
        print(f"   Hard negatives: {len(sample_pair['hard_negatives'])}")

    print("\n" + "=" * 80)
    print("‚úÖ TRAINING PAIRS –î–õ–Ø STAGE 1 –ì–û–¢–û–í–Ü (–∞–±–æ –æ–Ω–æ–≤–ª–µ–Ω—ñ)!")
    print("=" * 80)
    print(f"\nüéØ NEXT STEP: Fine-tuning E5-base –Ω–∞ training_pairs_stage1.jsonl")


if __name__ == "__main__":
    main()
