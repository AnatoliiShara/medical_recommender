"""
02_sample_drugs_for_training.py

–†–æ–∑—É–º–Ω–∏–π stratified sampling –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó training data.

–¶—ñ–ª—ñ:
- –ü–æ–∫—Ä–∏—Ç–∏ —Ä—ñ–∑–Ω—ñ —Ñ–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω—ñ –≥—Ä—É–ø–∏ (diversity).
- –ñ–æ—Ä—Å—Ç–∫–æ —Ç—Ä–∏–º–∞—Ç–∏ –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤ —É –¥—ñ–∞–ø–∞–∑–æ–Ω—ñ [target_min, target_max].
- –ê–∫—É—Ä–∞—Ç–Ω–æ –ø–æ–≤–æ–¥–∏—Ç–∏—Å—è –∑ –ø–æ—Ä–æ–∂–Ω—ñ–º–∏ / –≤—ñ–¥—Å—É—Ç–Ω—ñ–º–∏ —Ñ–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∏–º–∏ –≥—Ä—É–ø–∞–º–∏.
- –û—Ü—ñ–Ω–∏—Ç–∏ –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ—ó –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó hard negatives (–∞–ª–µ –ù–ï –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ó—Ö —Ç—É—Ç).

–†–µ–∑—É–ª—å—Ç–∞—Ç:
- data/training/finetuning/compendium_sampled.parquet
- data/training/finetuning/sampling_stats.json
"""

from pathlib import Path
import pandas as pd
import numpy as np
import json


# -----------------------------
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ –¥–ª—è sampling
# -----------------------------
TARGET_MIN = 5000       # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤ —É —Å–µ–º–ø–ª—ñ
TARGET_MAX = 7000       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤ —É —Å–µ–º–ø–ª—ñ
SMALL_GROUP_THRESHOLD = 10   # –ì—Ä—É–ø–∏ –∑ ‚â§ 10 –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤ –±–µ—Ä–µ–º–æ –ø–æ–≤–Ω—ñ—Å—Ç—é
MIN_PER_GROUP = 5            # –ú—ñ–Ω—ñ–º—É–º –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤ –¥–ª—è –≤–µ–ª–∏–∫–æ—ó –≥—Ä—É–ø–∏

QUERIES_PER_DRUG = 7         # –ü–ª–∞–Ω: 7 –∑–∞–ø–∏—Ç—ñ–≤ –Ω–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç
HARD_NEG_PER_QUERY = 5       # –ü–ª–∞–Ω: –¥–æ 5 hard negatives –Ω–∞ –∑–∞–ø–∏—Ç (–±—É–¥–µ –≤ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É –µ—Ç–∞–ø—ñ)


def get_paths():
    """
    –û–±—á–∏—Å–ª—é—î–º–æ —à–ª—è—Ö–∏ –≤—ñ–¥–Ω–æ—Å–Ω–æ –∫–æ—Ä–µ–Ω—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—é.
    –¶–µ –¥–æ–∑–≤–æ–ª—è—î –∑–∞–ø—É—Å–∫–∞—Ç–∏ —Å–∫—Ä–∏–ø—Ç –∑ –±—É–¥—å-—è–∫–æ–≥–æ –ø–æ—Ç–æ—á–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥—É.
    """
    script_path = Path(__file__).resolve()
    repo_root = script_path.parents[2]  # .../medrx-search-mvp/

    data_raw_path = repo_root / "data" / "raw" / "compendium_all.parquet"
    training_dir = repo_root / "data" / "training" / "finetuning"
    training_dir.mkdir(parents=True, exist_ok=True)

    sampled_path = training_dir / "compendium_sampled.parquet"
    stats_path = training_dir / "sampling_stats.json"

    return data_raw_path, sampled_path, stats_path


def normalize_group_column(df: pd.DataFrame) -> pd.Series:
    """
    –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –∫–æ–ª–æ–Ω–∫—É '–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞':
    - NaN ‚Üí ''
    - strip() –ø—Ä–æ–±—ñ–ª–∏
    - –ø–æ—Ä–æ–∂–Ω—ñ —Å—Ç—Ä–æ–∫–∏ ‚Üí NaN
    - NaN ‚Üí 'Unknown'
    """
    col = df["–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞"].fillna("").astype(str).str.strip()
    col = col.replace("", np.nan).fillna("Unknown")
    return col


def stratified_sample(
    df_valid: pd.DataFrame,
    target_min: int,
    target_max: int,
    random_state: int = 42,
) -> pd.DataFrame:
    """
    Stratified sampling –∑–∞ —Ñ–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∏–º–∏ –≥—Ä—É–ø–∞–º–∏ –∑ –∂–æ—Ä—Å—Ç–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º
    –¥—ñ–∞–ø–∞–∑–æ–Ω—É [target_min, target_max].

    –ê–ª–≥–æ—Ä–∏—Ç–º:
      1. –†–∞—Ö—É—î–º–æ –±–∞–∑–æ–≤—É —á–∞—Å—Ç–∫—É base_fraction = target_max / len(df_valid).
      2. –î–ª—è –º–∞–ª–∏—Ö –≥—Ä—É–ø (‚â§ SMALL_GROUP_THRESHOLD) –±–µ—Ä–µ–º–æ –≤—Å—ñ.
      3. –î–ª—è –≤–µ–ª–∏–∫–∏—Ö –≥—Ä—É–ø –±–µ—Ä–µ–º–æ ~count * base_fraction, –∞–ª–µ –Ω–µ –º–µ–Ω—à–µ MIN_PER_GROUP.
      4. –ü—ñ—Å–ª—è –ø–µ—Ä–≤–∏–Ω–Ω–æ–≥–æ stratified sampling:
         - —è–∫—â–æ > target_max ‚Üí –≤–∏–ø–∞–¥–∫–æ–≤–æ –¥–∞—É–Ω—Å–µ–º–ø–ª–∏–º–æ –¥–æ target_max;
         - —è–∫—â–æ < target_min ‚Üí –¥–æ–±–∏—Ä–∞—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤—ñ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∏ –∑ —Ä–µ—à—Ç–∏ df_valid
           –¥–æ target_min.
    """
    group_counts = df_valid["group"].value_counts()
    total_valid = len(df_valid)
    base_fraction = target_max / total_valid

    print(f"\nüìä –í—Å—å–æ–≥–æ –≤–∞–ª—ñ–¥–Ω–∏—Ö –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤: {total_valid:,}")
    print(f"üìä –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∏—Ö –≥—Ä—É–ø: {len(group_counts):,}")
    print(f"‚öñÔ∏è  –ë–∞–∑–æ–≤–∞ —á–∞—Å—Ç–∫–∞ –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –≥—Ä—É–ø: {base_fraction:.3f}")

    sampled_indices = []

    for group, count in group_counts.items():
        group_df = df_valid[df_valid["group"] == group]

        if count <= SMALL_GROUP_THRESHOLD:
            # –ú–∞–ª—ñ –≥—Ä—É–ø–∏ ‚Äî –±–µ—Ä–µ–º–æ –ø–æ–≤–Ω—ñ—Å—Ç—é
            sample_size = count
        else:
            # –í–µ–ª–∏–∫—ñ –≥—Ä—É–ø–∏ ‚Äî –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–∏–π sampling
            sample_size = int(round(count * base_fraction))
            sample_size = max(MIN_PER_GROUP, min(sample_size, count))

        sample = group_df.sample(
            n=sample_size,
            random_state=random_state,
            replace=False,
        )
        sampled_indices.extend(sample.index.tolist())

    # –ü–µ—Ä–≤–∏–Ω–Ω–∏–π —Å–µ–º–ø–ª
    df_sampled = df_valid.loc[sampled_indices].copy()

    # –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑–∞ –Ω–∞–∑–≤–æ—é –ø—Ä–µ–ø–∞—Ä–∞—Ç—É (–Ω–∞ –≤—Å—è–∫ –≤–∏–ø–∞–¥–æ–∫)
    df_sampled = df_sampled.drop_duplicates(subset=["–ù–∞–∑–≤–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—É"])

    print(f"\nüîÅ –ü—ñ—Å–ª—è –ø–µ—Ä–≤–∏–Ω–Ω–æ–≥–æ stratified sampling:")
    print(f"   –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤: {len(df_sampled):,}")

    # –ñ–æ—Ä—Å—Ç–∫–æ –∑–∞–±–µ–∑–ø–µ—á—É—î–º–æ –¥—ñ–∞–ø–∞–∑–æ–Ω [target_min, target_max]
    if len(df_sampled) > target_max:
        # –î–∞—É–Ω—Å–µ–º–ø–ª–∏–º–æ –¥–æ target_max
        df_sampled = df_sampled.sample(
            n=target_max,
            random_state=random_state,
            replace=False,
        )
        print(f"   üîª –î–∞—É–Ω—Å–µ–º–ø–ª –¥–æ TARGET_MAX = {target_max:,}")
    elif len(df_sampled) < target_min:
        # –î–æ–±–∏—Ä–∞—î–º–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∏ –∑ —Ä–µ—à—Ç–∏ df_valid
        deficit = target_min - len(df_sampled)
        print(f"   üî∫ –ú–∞–ª–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤, –¥–æ–±–∏—Ä–∞—î–º–æ —â–µ: {deficit:,}")

        remaining = df_valid.drop(index=df_sampled.index)
        add_size = min(deficit, len(remaining))

        if add_size > 0:
            extra = remaining.sample(
                n=add_size,
                random_state=random_state,
                replace=False,
            )
            df_sampled = pd.concat([df_sampled, extra], axis=0)

    # –§—ñ–Ω–∞–ª—å–Ω–∞ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ—Å—Ç—å –∑–∞ –Ω–∞–∑–≤–æ—é
    df_sampled = df_sampled.drop_duplicates(subset=["–ù–∞–∑–≤–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—É"])

    print(f"\n‚úÖ –§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä —Å–µ–º–ø–ª—É: {len(df_sampled):,} –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤")
    return df_sampled


def sample_drugs_stratified():
    print("=" * 80)
    print("üéØ SAMPLING –ü–†–ï–ü–ê–†–ê–¢–Ü–í –î–õ–Ø TRAINING (stratified –∑–∞ —Ñ–∞—Ä–º–∞–∫–æ–≥—Ä—É–ø–∞–º–∏)")
    print("=" * 80)

    data_raw_path, sampled_path, stats_path = get_paths()

    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è full dataset
    print(f"\nüìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ Compendium –∑: {data_raw_path}")
    df = pd.read_parquet(data_raw_path)
    print(f"   –í—Å—å–æ–≥–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤: {len(df):,}")

    # 2. –§—ñ–ª—å—Ç—Ä: —Ç—ñ–ª—å–∫–∏ –∑ –ø–æ–∫–∞–∑–∞–Ω–Ω—è–º–∏
    df_valid = df[df["–ü–æ–∫–∞–∑–∞–Ω–Ω—è"].notna()].copy()
    print(f"‚úÖ –ü—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤ –∑ –Ω–µ–ø–æ—Ä–æ–∂–Ω—ñ–º–∏ '–ü–æ–∫–∞–∑–∞–Ω–Ω—è': {len(df_valid):,}")

    # 3. –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ñ–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω—ñ –≥—Ä—É–ø–∏
    df_valid["group"] = normalize_group_column(df_valid)

    # 4. Stratified sampling –∑ –∂–æ—Ä—Å—Ç–∫–∏–º –¥—ñ–∞–ø–∞–∑–æ–Ω–æ–º
    df_sampled = stratified_sample(
        df_valid=df_valid,
        target_min=TARGET_MIN,
        target_max=TARGET_MAX,
        random_state=42,
    )

    # 5. Quality checks
    avg_indications_len = df_sampled["–ü–æ–∫–∞–∑–∞–Ω–Ω—è"].fillna("").str.len().mean()
    groups_covered = df_sampled["group"].nunique()

    print(f"\nüîç Quality checks:")
    print(f"   –°–µ—Ä–µ–¥–Ω—è –¥–æ–≤–∂–∏–Ω–∞ '–ü–æ–∫–∞–∑–∞–Ω–Ω—è': {avg_indications_len:.0f} —Å–∏–º–≤–æ–ª—ñ–≤")
    print(f"   –ü–æ–∫—Ä–∏—Ç–æ —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∏—Ö –≥—Ä—É–ø: {groups_covered:,}")

    # 6. –û—Ü—ñ–Ω–∫–∞ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ–≥–æ training data
    expected_queries = len(df_sampled) * QUERIES_PER_DRUG
    expected_positive_pairs = expected_queries
    expected_pairs_with_hn = expected_queries * (1 + HARD_NEG_PER_QUERY)

    print(f"\nüìà –ü–æ—Ç–µ–Ω—Ü—ñ–∞–ª –¥–ª—è training data:")
    print(f"   üîπ Queries (‚âà{QUERIES_PER_DRUG} –Ω–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç): {expected_queries:,}")
    print(f"   üîπ –ü–æ–∑–∏—Ç–∏–≤–Ω—ñ –ø–∞—Ä–∏ (query‚Äìpositive_passage): {expected_positive_pairs:,}")
    print(
        f"   üîπ –ú–∞–∫—Å. –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª –∑ hard negatives "
        f"(+{HARD_NEG_PER_QUERY} –Ω–∞ query): –¥–æ {expected_pairs_with_hn:,} –ø–∞—Ä"
    )
    print("   ‚ö†Ô∏è Hard negatives –ë–£–î–£–¢–¨ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É –µ—Ç–∞–ø—ñ "
          "(03_build_training_pairs.py). –£ —Ü—å–æ–º—É —Å–∫—Ä–∏–ø—Ç—ñ –º–∏ –ª–∏—à–µ —Å–µ–º–ø–ª–∏–º–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∏.")

    # 7. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–µ–º–ø–ª—É
    df_sampled.to_parquet(sampled_path)
    print(f"\nüíæ Sampled dataset –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {sampled_path}")

    # 8. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats = {
        "total_drugs": int(len(df)),
        "valid_drugs": int(len(df_valid)),
        "sampled_drugs": int(len(df_sampled)),
        "coverage_pct": float(len(df_sampled) / len(df_valid) * 100.0),
        "therapeutic_groups_total": int(df_valid["group"].nunique()),
        "therapeutic_groups_covered": int(groups_covered),
        "target_min": int(TARGET_MIN),
        "target_max": int(TARGET_MAX),
        "queries_per_drug": int(QUERIES_PER_DRUG),
        "hard_neg_per_query": int(HARD_NEG_PER_QUERY),
        "expected_queries": int(expected_queries),
        "expected_positive_pairs": int(expected_positive_pairs),
        "expected_pairs_with_hard_negatives": int(expected_pairs_with_hn),
    }

    with stats_path.open("w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)

    print(f"üìä Statistics –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {stats_path}")

    print("\n" + "=" * 80)
    print("‚úÖ SAMPLING –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 80)

    return df_sampled


if __name__ == "__main__":
    df_sampled = sample_drugs_stratified()

    print(f"\nüéØ NEXT STEP:")
    print(f"   –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –ø–∞—Ü—ñ—î–Ω—Ç—Å—å–∫—ñ –∑–∞–ø–∏—Ç–∏ –¥–ª—è {len(df_sampled):,} –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤")
    print(f"   (–Ω–∞–ø—Ä. —á–µ—Ä–µ–∑ Gemini API) —ñ –ø–æ–±—É–¥—É–≤–∞—Ç–∏ training pairs –¥–ª—è —Ñ–∞–π–Ω—Ç—é–Ω—ñ–Ω–≥—É.")
