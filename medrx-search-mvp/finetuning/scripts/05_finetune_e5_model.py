"""
05_finetune_e5_model.py

Fine-tuning intfloat/multilingual-e5-base –¥–ª—è Ukrainian medical search (Stage 1)
- CPU-optimized –ø—ñ–¥ –Ω–æ—É—Ç –∑ 12 –ª–æ–≥—ñ—á–Ω–∏–º–∏ —è–¥—Ä–∞–º–∏, 15 GiB RAM.
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î training_pairs_stage1.jsonl:
    {"query": ..., "positive": ..., "hard_negatives": [...]}

–û—Å–Ω–æ–≤–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è:
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ MultipleNegativesRankingLoss –∑ –ø–∞—Ä–∞–º–∏ (query, positive).
  Hard negatives —Å–ª—É–≥—É—é—Ç—å –¥–æ–¥–∞—Ç–∫–æ–≤–∏–º –¥–∂–µ—Ä–µ–ª–æ–º "–≤–∞–∂–∫–∏—Ö" –ø—Ä–∏–∫–ª–∞–¥—ñ–≤,
  –∞–ª–µ –≤ Stage 1 –º–∏ –ø–æ–∫–ª–∞–¥–∞—î–º–æ—Å—è –Ω–∞ in-batch negatives (–∫–ª–∞—Å–∏—á–Ω–∞ —Å—Ö–µ–º–∞ E5).
- –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å CPU-—Ç—Ä–µ–¥—ñ–≤, —â–æ–± –Ω–æ—É—Ç –∑–∞–ª–∏—à–∞–≤—Å—è –∂–∏–≤–∏–º.
- –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ resume —á–µ—Ä–µ–∑ --resume_from <checkpoint_dir>.
"""

from __future__ import annotations

import os
import json
import logging
from pathlib import Path
from datetime import datetime
import argparse
import random

import torch
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, InputExample, losses

# -------------------------------------------------------
# –ë–∞–∑–æ–≤—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
# -------------------------------------------------------

RANDOM_SEED = 42


def setup_cpu_threads(num_threads: int = 6) -> None:
    """
    –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤ –¥–ª—è BLAS / PyTorch, —â–æ–± –º–∞—à–∏–Ω–∞ –Ω–µ –∑–∞–¥–∏—Ö–∞–ª–∞—Å—å.
    """
    env_vars = [
        "OMP_NUM_THREADS",
        "MKL_NUM_THREADS",
        "OPENBLAS_NUM_THREADS",
        "NUMEXPR_NUM_THREADS",
        "PYTORCH_NUM_THREADS",
    ]
    for var in env_vars:
        os.environ.setdefault(var, str(num_threads))

    try:
        torch.set_num_threads(num_threads)
    except Exception:
        pass

    try:
        torch.set_num_interop_threads(max(1, num_threads // 2))
    except Exception:
        pass


def get_repo_root() -> Path:
    """
    –ó–Ω–∞—Ö–æ–¥–∏–º–æ –∫–æ—Ä—ñ–Ω—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—é –≤—ñ–¥–Ω–æ—Å–Ω–æ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Ñ–∞–π–ª—É.
    """
    script_path = Path(__file__).resolve()
    # finetuning/scripts/05_finetune_e5_model.py -> repo_root = parents[2]
    return script_path.parents[2]


def setup_logging(logs_dir: Path) -> logging.Logger:
    logs_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = logs_dir / f"e5_stage1_{timestamp}.log"

    logger = logging.getLogger("finetune_e5_stage1")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    formatter = logging.Formatter(
        "%(asctime)s - %(levelname)s - %(message)s", "%Y-%m-%d %H:%M:%S"
    )

    fh = logging.FileHandler(log_file, encoding="utf-8")
    fh.setLevel(logging.INFO)
    fh.setFormatter(formatter)

    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(formatter)

    logger.addHandler(fh)
    logger.addHandler(ch)

    logger.info(f"üìÑ Logging to: {log_file}")
    return logger


# -------------------------------------------------------
# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
# -------------------------------------------------------

def load_training_data(path: Path, logger: logging.Logger) -> list[InputExample]:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ training_pairs_stage1.jsonl —ñ –±—É–¥—É—î–º–æ InputExample'–∏
    –¥–ª—è MultipleNegativesRankingLoss —è–∫ –ø–∞—Ä–∏ (query, positive).

    hard_negatives –Ω–∞ —Ü—å–æ–º—É –µ—Ç–∞–ø—ñ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —è–≤–Ω–æ, –∞–ª–µ –≤–æ–Ω–∏ –≤–∂–µ
    –≤—Ä–∞—Ö–æ–≤–∞–Ω—ñ –ø—Ä–∏ –ø–æ–±—É–¥–æ–≤—ñ training_pairs_stage1 (—á–µ—Ä–µ–∑ sampling).
    """
    logger.info(f"üìÇ Loading training data from: {path}")
    if not path.exists():
        raise FileNotFoundError(f"Training data not found: {path}")

    examples: list[InputExample] = []

    with path.open("r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            data = json.loads(line)

            query = str(data.get("query", "")).strip()
            positive = str(data.get("positive", "")).strip()
            if not query or not positive:
                continue

            # –î–ª—è MNRL: —Ç—ñ–ª—å–∫–∏ (anchor, positive)
            examples.append(InputExample(texts=[query, positive]))

    logger.info(f"‚úÖ Loaded {len(examples):,} training examples")
    return examples


# -------------------------------------------------------
# –û—Å–Ω–æ–≤–Ω–∏–π —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω
# -------------------------------------------------------

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Fine-tune intfloat/multilingual-e5-base on medical search data (Stage 1)."
    )
    parser.add_argument(
        "--epochs", type=int, default=3, help="Number of epochs (default: 3)"
    )
    parser.add_argument(
        "--batch_size", type=int, default=16, help="Batch size (default: 16)"
    )
    parser.add_argument(
        "--learning_rate", type=float, default=2e-5, help="Learning rate (default: 2e-5)"
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=0,
        help="DataLoader workers (default: 0, —â–æ–± –Ω–µ –ø–ª–æ–¥–∏—Ç–∏ –∑–∞–π–≤—ñ –ø—Ä–æ—Ü–µ—Å–∏)",
    )
    parser.add_argument(
        "--resume_from",
        type=str,
        default=None,
        help="Path to checkpoint dir to resume from (optional)",
    )
    return parser.parse_args()


def main():
    setup_cpu_threads(num_threads=6)

    repo_root = get_repo_root()
    logs_dir = repo_root / "logs" / "finetuning"
    logger = setup_logging(logs_dir)

    logger.info("=" * 80)
    logger.info("üöÄ FINE-TUNING intfloat/multilingual-e5-base (Stage 1: retrieval)")
    logger.info("=" * 80)

    args = parse_args()
    random.seed(RANDOM_SEED)
    torch.manual_seed(RANDOM_SEED)

    # –®–ª—è—Ö–∏
    base_model_name = "intfloat/multilingual-e5-base"
    train_data_path = repo_root / "data" / "training" / "finetuning" / "training_pairs_stage1.jsonl"
    output_dir = repo_root / "models" / "finetuned" / "e5-medrx-stage1"
    checkpoint_dir = output_dir / "checkpoints"
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    output_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"üìÅ Repo root: {repo_root}")
    logger.info(f"üìÅ Training data: {train_data_path}")
    logger.info(f"üìÅ Output dir: {output_dir}")
    logger.info(f"üìÅ Checkpoints dir: {checkpoint_dir}")

    # –ü—Ä–∏—Å—Ç—Ä—ñ–π
    device = torch.device("cpu")
    logger.info(f"üñ•Ô∏è  Device: {device}")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ (base –∞–±–æ checkpoint)
    if args.resume_from:
        resume_path = Path(args.resume_from).resolve()
        if not resume_path.exists():
            raise FileNotFoundError(f"Checkpoint for resume not found: {resume_path}")
        logger.info(f"üì• Resuming from checkpoint: {resume_path}")
        model = SentenceTransformer(str(resume_path), device=str(device))
    else:
        logger.info(f"üì• Loading base model: {base_model_name}")
        model = SentenceTransformer(base_model_name, device=str(device))

    num_params = sum(p.numel() for p in model.parameters())
    logger.info(f"üìä Model parameters: {num_params:,}")
    logger.info(f"üìä Embedding dimension: {model.get_sentence_embedding_dimension()}")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    examples = load_training_data(train_data_path, logger)

    # DataLoader
    train_dataloader = DataLoader(
        examples,
        shuffle=True,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=False,
        drop_last=True,
    )

    steps_per_epoch = len(train_dataloader)
    total_steps = steps_per_epoch * args.epochs
    warmup_steps = max(100, int(0.1 * total_steps))  # ‚âà10% warmup

    logger.info("\nüìä Training configuration:")
    logger.info(f"   Epochs: {args.epochs}")
    logger.info(f"   Batch size: {args.batch_size}")
    logger.info(f"   Steps per epoch: {steps_per_epoch:,}")
    logger.info(f"   Total steps: {total_steps:,}")
    logger.info(f"   Warmup steps (~10%): {warmup_steps}")
    logger.info(f"   Learning rate: {args.learning_rate}")
    logger.info(f"   DataLoader workers: {args.num_workers}")
    logger.info(f"   CPU threads (PyTorch): {torch.get_num_threads()}")

    # Loss
    train_loss = losses.MultipleNegativesRankingLoss(model)

    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
    config_dict = {
        "base_model": base_model_name,
        "output_dir": str(output_dir),
        "checkpoint_dir": str(checkpoint_dir),
        "epochs": args.epochs,
        "batch_size": args.batch_size,
        "learning_rate": args.learning_rate,
        "total_examples": len(examples),
        "steps_per_epoch": steps_per_epoch,
        "total_steps": total_steps,
        "warmup_steps": warmup_steps,
        "resume_from": args.resume_from,
        "training_started_at": datetime.now().isoformat(),
    }
    config_path = output_dir / "training_config_stage1.json"
    with config_path.open("w", encoding="utf-8") as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    logger.info(f"üíæ Saved training config to: {config_path}")

    logger.info("\n‚è∞ Training started...")
    logger.info("=" * 80)

    try:
        model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            epochs=args.epochs,
            warmup_steps=warmup_steps,
            output_path=str(output_dir),
            optimizer_params={"lr": args.learning_rate},
            show_progress_bar=True,
            use_amp=False,  # CPU: AMP –Ω–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω
            checkpoint_path=str(checkpoint_dir),
            checkpoint_save_steps=1000,
            checkpoint_save_total_limit=5,
        )

        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ TRAINING COMPLETED (Stage 1)!")
        logger.info("=" * 80)
        logger.info(f"‚è∞ Finished at: {datetime.now().isoformat()}")
        logger.info(f"üíæ Final model saved to: {output_dir}")

    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è Training interrupted by user (KeyboardInterrupt).")
        logger.info(f"üíæ Checkpoints available in: {checkpoint_dir}")
    except Exception as e:
        logger.error(f"\n‚ùå Training failed with exception: {e}")
        raise


if __name__ == "__main__":
    main()
