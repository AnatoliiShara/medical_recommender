"""
EDA –¥–ª—è MedRx compendium data - –∞–Ω–∞–ª—ñ–∑ –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª—É –¥–ª—è —Ñ–∞–π–Ω—Ç—é–Ω—ñ–Ω–≥—É
"""
import pandas as pd
import numpy as np
from pathlib import Path
import json
from collections import Counter
import re

def analyze_compendium():
    """–ê–Ω–∞–ª—ñ–∑ compendium dataset"""
    
    print("="*80)
    print("üìä –ê–ù–ê–õ–Ü–ó COMPENDIUM DATASET –î–õ–Ø –§–ê–ô–ù–¢–Æ–ù–Ü–ù–ì–£")
    print("="*80)
    
    # Load data
    parquet_path = "data/raw/compendium_all.parquet"
    print(f"\nüìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {parquet_path}")
    
    df = pd.read_parquet(parquet_path)
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df):,} –∑–∞–ø–∏—Å—ñ–≤")
    
    # Basic info
    print(f"\nüìã –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–∏—Ö:")
    print(f"   –ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
    print(f"   –ü–∞–º'—è—Ç—å: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
    
    # Check key columns
    key_columns = ['–ù–∞–∑–≤–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—É', '–ü–æ–∫–∞–∑–∞–Ω–Ω—è', '–ü—Ä–æ—Ç–∏–ø–æ–∫–∞–∑–∞–Ω–Ω—è', 
                   '–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞', '–°–∫–ª–∞–¥']
    
    print(f"\nüîç –ö–ª—é—á–æ–≤—ñ –ø–æ–ª—è (–¥–ª—è training data):")
    for col in key_columns:
        if col in df.columns:
            non_null = df[col].notna().sum()
            pct = (non_null / len(df)) * 100
            
            if non_null > 0:
                avg_len = df[col].dropna().astype(str).str.len().mean()
                print(f"   {col:35s}: {non_null:5,} ({pct:5.1f}%) | avg {avg_len:4.0f} chars")
            else:
                print(f"   {col:35s}: {non_null:5,} ({pct:5.1f}%)")
    
    # Analyze –ü–æ–∫–∞–∑–∞–Ω–Ω—è (main source for queries)
    if '–ü–æ–∫–∞–∑–∞–Ω–Ω—è' in df.columns:
        indications = df['–ü–æ–∫–∞–∑–∞–Ω–Ω—è'].dropna().astype(str)
        
        print(f"\nüíä –ê–Ω–∞–ª—ñ–∑ –ø–æ–ª—è '–ü–æ–∫–∞–∑–∞–Ω–Ω—è' (–¥–∂–µ—Ä–µ–ª–æ –¥–ª—è queries):")
        print(f"   –ü—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤ –∑ –ø–æ–∫–∞–∑–∞–Ω–Ω—è–º–∏: {len(indications):,}")
        print(f"   –°–µ—Ä–µ–¥–Ω—è –¥–æ–≤–∂–∏–Ω–∞: {indications.str.len().mean():.0f} —Å–∏–º–≤–æ–ª—ñ–≤")
        print(f"   –ú–µ–¥—ñ–∞–Ω–∞ –¥–æ–≤–∂–∏–Ω–∏: {indications.str.len().median():.0f} —Å–∏–º–≤–æ–ª—ñ–≤")
        
        # Sample indication
        sample = indications.iloc[0]
        print(f"\n   –ü—Ä–∏–∫–ª–∞–¥ –ø–æ–∫–∞–∑–∞–Ω–Ω—è:")
        print(f"   {sample[:200]}...")
    
    # Training data potential
    drugs_with_indications = df['–ü–æ–∫–∞–∑–∞–Ω–Ω—è'].notna().sum()
    queries_per_drug = 7  # –ë—É–¥–µ–º–æ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ 7 queries –Ω–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç
    
    print(f"\nüìà –ü–û–¢–ï–ù–¶–Ü–ê–õ –î–õ–Ø TRAINING DATA:")
    print(f"   –ü—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤ –∑ –ø–æ–∫–∞–∑–∞–Ω–Ω—è–º–∏: {drugs_with_indications:,}")
    print(f"   Queries –Ω–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç: {queries_per_drug}")
    print(f"   –û—á—ñ–∫—É–≤–∞–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å queries: {drugs_with_indications * queries_per_drug:,}")
    print(f"   Training pairs (–∑ negatives): ~{drugs_with_indications * queries_per_drug:,}")
    
    # Therapeutic groups distribution
    if '–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞' in df.columns:
        groups = df['–§–∞—Ä–º–∞–∫–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∞ –≥—Ä—É–ø–∞'].dropna()
        top_groups = groups.value_counts().head(10)
        
        print(f"\nüè• –¢–æ–ø-10 —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–Ω–∏—Ö –≥—Ä—É–ø:")
        for i, (group, count) in enumerate(top_groups.items(), 1):
            pct = (count / len(df)) * 100
            print(f"   {i:2d}. {group[:50]:50s} | {count:4d} ({pct:.1f}%)")
    
    # Save summary
    summary = {
        "total_drugs": len(df),
        "drugs_with_indications": int(drugs_with_indications),
        "estimated_training_queries": int(drugs_with_indications * queries_per_drug),
        "columns": list(df.columns)
    }
    
    output_path = "data/interim/eda/compendium_analysis.json"
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {output_path}")
    
    print("\n" + "="*80)
    print("‚úÖ –ê–ù–ê–õ–Ü–ó –ó–ê–í–ï–†–®–ï–ù–û")
    print("="*80)
    print(f"\nüéØ –í–∏—Å–Ω–æ–≤–æ–∫: –ú–æ–∂–µ–º–æ –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ ~{drugs_with_indications * queries_per_drug:,} training pairs")
    print(f"   –¶–µ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–ª—è —è–∫—ñ—Å–Ω–æ–≥–æ —Ñ–∞–π–Ω—Ç—é–Ω—ñ–Ω–≥—É!")

if __name__ == "__main__":
    analyze_compendium()
